{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobre la tarea anterior\n",
    "En la tarea anterior, no se consideró correctamente la clase a la que pertenecía cada corpus, lo que llevó a un enfoque incompleto. Para corregirlo, se ha repetido el proceso siguiendo los mismos pasos, pero ahora incluyendo explícitamente la clase correspondiente. Esto asegura que todos los elementos y procedimientos se ajusten al contexto adecuado.\n",
    "\n",
    "Lo que se ha reutilizado en la tarea es el proceso de eliminación de las cabeceras, ya que se han introducido directamente los ficheros modificados sin necesidad de realizar esa parte del trabajo nuevamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importacion de corpus sin las cabeceras\n",
    "Se ha realizado en la Tarea 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd() + \"/Corpus-representacion\"\n",
    "# Obtener el listado\n",
    "listado = os.listdir(path)\n",
    "corpus_dir = [] # Guardamos el directorio para cada fichero\n",
    "for elemento in listado:\n",
    "    corpus_dir.append(path +'/'+elemento)\n",
    "\n",
    "all_corpus_files = []\n",
    "for dir in corpus_dir:\n",
    "    corpus_files = os.listdir(dir)\n",
    "    for corpus in corpus_files:\n",
    "        all_corpus_files.append(dir +'/'+corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talk.politics.mideast',\n",
       " 'rec.autos',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.electronics',\n",
       " 'talk.politics.guns']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(ruta_fichero):\n",
    "    \"\"\"\n",
    "    Lee el contenido de un fichero de texto y lo retorna como un string único.\n",
    "    \n",
    "    Args:\n",
    "        ruta_fichero (str): Ruta al archivo de texto.\n",
    "        \n",
    "    Returns:\n",
    "        str: Contenido completo del fichero como una única cadena.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(ruta_fichero, 'r', encoding='utf-8') as fichero:\n",
    "            contenido = fichero.read()\n",
    "        return contenido\n",
    "    except FileNotFoundError:\n",
    "        return \"Error: El fichero no se encuentra.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for corpus in all_corpus_files:\n",
    "    for tema in listado:\n",
    "        if tema in corpus:\n",
    "            data.append((read_file(corpus), tema))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(805, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['content', 'class'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n",
    "En este apartado se aplican los mismos filtros que en la tarea 3, pero se generara un dataframe que a cada elemento le correspondera una classe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Message-ID: <FLAX.93Apr6125933@frej.teknikum.uu.se>References: <1993Mar30.142700.543@vms.huji.ac.il> <FLAX.93Apr3142133@frej.teknikum.uu.se><FLAX.93Apr5224449@frej.teknikum.uu.se><1993Apr5.221759.28472@thunder.mcrcim.mcgill.edu>NNTP-Posting-Host: frej.teknikum.uu.seIn-reply-to: hasan@McRCIM.McGill.EDU \\'s message of Mon, 5 Apr 93 22:17:59 GMTIn article <1993Apr5.221759.28472@thunder.mcrcim.mcgill.edu> hasan@McRCIM.McGill.EDU  writes:[ stuff deleted ]|> I wrote:|> Are you calling names, or giving me a title? If the first, read your|> paragraph above, if not I accept the title, in order to let you get into the|> um, well, debate again.Hasan replies:I didnot know that \"Master of wisdom\" can be \"name clling\" too,unless you consider yourself deserve-less !Unless you are referring to someone else, you have in fact given me a nameI did not ask for, hence the term \\'name calling\\'.Hasan writes:|>    So what do you expect me to tell you to tell you, Master of Wsidom,|> \\t\\t\\t\\t\\t\\t\\t       ^^^|> ------------------------------------------------------------------I replied:|> If you insist on giving me names/titles I did not ask for you could at|> least spell them correctly. /sigh.Hasan gloats:That was only to confuse you! (ha ha ha hey )Hell-bent on retarding into childhood, no?|>when you are intentionally neglecting the MOST important fact that|>the whole israeli presence in the occupied territories is ILLEGITIMATE,|>and hence ALL their actions, their courts, their laws are illegitimate on|>the ground of occupied territories.|>>No, I am _not_ neglecting that, I\\'m merely asking you whether the existance>of Israeli citicens in the WB or in Gaza invalidates those individuals>right^^^^^^^ are you trying to retaliate and confuse me here.No, I really do try to spell correctly, and I apologize if I did confuse you.I will try not to repeat that.|> to live, a (as you so eloquently put it) human right. We can get back to the|> question of which law should be used in the territories later. Also, you have|> not adressed my question if the israelis also have human rights.First, my above statement doesnot say that \"the existence of israeli citizensin the WB revoke their right of life\" but it says \"the israeli occupationof the WB revoke the right of life for some/most its citizens - basicallyrevokes the right of for its military men\". Clearly, occupation is anundeclared war; during war, attacks against military targets are fully legitimate.Ok, let me re-phrase the question. I have repeatedly asked you if theIsraelis have less human rights than the palestinians, and if so, why.From your posting (where you did not directly adress my question) I inferredthat you thought so. Together with the above statement I then assumed that thereason was the actions of the state of Israel. Re: your statement ofoccupation: I\\'d like you to define the term, so I don\\'t have to repeat this\\'drag the answer out of hasan\\' procedure more than neccesary.Secondly, surely israeli have human rights, but they ask their goverment toprotect it by withdrawing from the occupied terretories, not by further oppressingPalestinean human rights.I\\'m sorry, but the above sentence does not make sense. Please rephrase it.|> If a state can deprive all it\\'s citizens of human rights by its actions, then|> tell me why _any_ human living today should have any rights at all?Because not all states are like Israel, as oppressive, as ignorant, or as tyrant.Oh, ok. So how about the human rights of the Syrians, Iraqis and others?Does the name of Hama sound familiar? Or how about the kurds in Iraq andTurkey?How about the Same in Sweden (Ok, maybe a bit farfetched..) the Russians inthe Baltic states or the Moslem in the old USSR and Yugoslavia?Do the serbs have any human rights remainaing, according to you?|>    |> And which system do you propose we use to solve the ME problem?|>|>    The question is NOT which system would solve the ME problem. Why ? because|>    any system can solve it.|>    The laws of minister Sharon says kick Palestineans out of here (all palestine).|>|> I asked for which system should be used, that will preserve human rights for^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^|> all people involved. I assumed that was obvious, but I won\\'t repeat that|> mistake. Now that I have straightened that out, I\\'m eagerly awaiting your|> reply.So you agree that that an israeli solution wouldnot preserve human rights.(i am understanding this from your first statement in this paragraph).No, I\\'m agreeing that to just kick all the Palestinians out of Israel properwould probably lead to disaster for both parties. If that\\'s what you referto as the \\'Israeli solution\\' then so be it.|>    Joseph Weitz (administrator responsible for Jewish colonization)|>    said it best when writing in his diary in 1940:|> \\t   \"Between ourselves it must be clear that there is no room for both|> \\t   peoples together in this country.... We shall not achieve our goal|> \\t\\t\\t\\t\\t\\t^^^                  ^^^|> \\t   of being an independent people with the Arabs in this small country.|> \\t   The only solution is a Palestine, at least Western Palestine (west of|> \\t   the Jordan river) without Arabs.... And there is no other way than|> \\t   to transfer the Arabs from here to the neighbouring countries, to|> \\t   transfer all of them; not one village, not one tribe, should be|> \\t   left.... Only after this transfer will the country be able to|> \\t   absorb the millions of our own brethren. There is no other way out.\"|> \\t\\t\\t\\t   DAVAR, 29 September, 1967|> \\t\\t\\t\\t   (\"Courtesy\" of Marc Afifi)|>|> Just a question: If we are to disregard the rather obvious references to|> getting Israel out of ME one way or the other in both PLO covenant and HAMAS|> charter (that\\'s the english translations, if you have other information I\\'d|> be interested to have you translate it) why should we give any credence to|> a _private_ paper even older? I\\'m not going to get into the question if he|> wrote the above, but it\\'s fairly obvious all parties in the conflict have|> their share of fanatics. Guess what..? Those are not the people that will|> make any lasting peace in the region. [more deleted stuff]>Exactly, you are right. I guess that the problem is that the israeli goverment>is full with  men like Joseph Weitz.Oh? Have you met with them personally, to read their diaries? Fascinating.What do you _do_ for a living?|>    \"We\" and \"our\" either refers to Zionists or Jews (i donot know which).|>|>    Well, i can give you an answer, you Master of Wisdom, I will NOT suggest the|>    imperialist israeli system for solving the ME problem !|>|>    I think that is fair enough .|>|> No, that is _not_ an answer, since I asked for a system that could solve|> the problem. You said any could be used, then you provided a contradiction.Above you wrote that you understood what i meant (underlined by ^ ):any system can be used to solve the conflict , but not any system wouldresolve it JUSTLY.An unjust solution would be a non-solution, per definition, no?You said the following:For all A it holds that A have property B.There exists an A such that property B does not hold.Thus, either or both statements must be false.|> Guess where that takes your logic? To never-never land.>You are proving yourself as a \" \". First you understood what i meant, but then>you claim you didnot so to claim a contradiction in my logic.>Too bad for you, the Master of Wisdom.I was merely pointing out a not so small flaw in your reasoning.Since you claim to be logical I felt it best to point this outbefore you started using your statements to prove a point or so.Am I then to assume you are  not logical?|>    \"The greatest problem of Zionism is Arab children\".|> \\t\\t\\t   -Rabbi Shoham.|>|> Oh, and by the way, let me add that these cute quotes you put at the end are|> a real bummer, when I try giving your posts any credit.>Why do you feel ashamed by things and facts that you believe in ,>if you were a Zionists. If you believe in Zionist codes and acts,>well i feel sorry for you, because the same Rabbi Shoham had said>\"Yes, Zionism is racism\".>If you feel ashamed and bothered by the Zionist codes, then drop Zionism.>If you are not Zionist, why are you bothered then. You should join me in>condemning these racist Zionist codes and acts.Any quote can be misused, especially when used to stereotype allindividuals by a statement of an individual. If you use the samemethods that you credit \\'Zionists\\' with, then where does that place you?Oh, by the way, I\\'d advice you not to assume anything about my \\'loyalties\\'.I will and am condemning acts I find vile and inhuman, but I\\'ll try aslong as I can not to assume those acts are by a whole people.By zionist above do you mean the state of Israel, the government of Israel,the leaders of Israel (political and/or religious) or the jews ingeneral? If you feel the need to condemn, condemn those responsibleinstead. How would you feel if we started condemning you personallybased on the bombings in Egypt?----------------------------------------------------------Jonas Flygare, \\t\\t+ Wherever you go, there you areV{ktargatan 32 F:621\\t+754 22 Uppsala, Sweden\\t+'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello small go go god afternoon a i'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words as nltk_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "# nltk.download('words')\n",
    "\n",
    "valid_words = set(nltk_words.words())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def convert_string_to_array(string_to_convert):\n",
    "    # Dividir la cadena en palabras, eliminando espacios en blanco\n",
    "    return [word for word in string_to_convert.split(' ') if word]\n",
    "\n",
    "def remove_all_words_that_contain_numbers(list_of_words):\n",
    "    # Verifica que cada elemento en la lista no contenga números\n",
    "    return [word for word in list_of_words if not re.search(r'\\d', word)]\n",
    "\n",
    "def filter_word_by_lenght(list_of_words):\n",
    "    exceptions = ['i', 'a']\n",
    "    return [word for word in list_of_words if 2 <= len(word) <= 15 or word in exceptions]\n",
    "\n",
    "def filter_stop_words(list_of_words):\n",
    "    filtered_bag_of_words = []\n",
    "    for word in list_of_words:\n",
    "        if word not in stop_words:\n",
    "            filtered_bag_of_words.append(word)\n",
    "    return filtered_bag_of_words\n",
    "\n",
    "def word_exists(list_of_words):\n",
    "    return [word for word in list_of_words if word in valid_words or word == '@@@']\n",
    "\n",
    "def array_to_string(list_of_words):\n",
    "    return ' '.join(list_of_words)\n",
    "\n",
    "# Función para mapear tipos gramaticales\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def apply_lemmatizer(list_of_words):\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in list_of_words]\n",
    "\n",
    "def preproces(data):\n",
    "    preproces_data = convert_string_to_array(data)\n",
    "    preproces_data = remove_all_words_that_contain_numbers(preproces_data)\n",
    "    preproces_data = filter_word_by_lenght(preproces_data)\n",
    "    preproces_data = word_exists(preproces_data)\n",
    "    preproces_data = filter_stop_words(preproces_data)\n",
    "    preproces_data = apply_lemmatizer(preproces_data)\n",
    "    return array_to_string(preproces_data)\n",
    "\n",
    "\n",
    "test = \"hello smaller macia2345 going gone asdfasdf45 god afternoon a i m\"\n",
    "test = convert_string_to_array(test)\n",
    "test = remove_all_words_that_contain_numbers(test)\n",
    "test = filter_word_by_lenght(test)\n",
    "test = word_exists(test)\n",
    "test = apply_lemmatizer(test)\n",
    "test = array_to_string(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Message-ID: &lt;FLAX.93Apr6125933@frej.teknikum.u...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In article &lt;1993Apr5.202800.27705@wam.umd.edu&gt;...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEPOSITION of VITALY NIKOLAYEVICH DANIELIAN [1...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nntp-Posting-Host: saluda.columbiasc.ncr.comIn...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In article &lt;iacovou.734063606@gurney&gt; iacovou@...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content                  class\n",
       "0  Message-ID: <FLAX.93Apr6125933@frej.teknikum.u...  talk.politics.mideast\n",
       "1  In article <1993Apr5.202800.27705@wam.umd.edu>...  talk.politics.mideast\n",
       "2  DEPOSITION of VITALY NIKOLAYEVICH DANIELIAN [1...  talk.politics.mideast\n",
       "3  Nntp-Posting-Host: saluda.columbiasc.ncr.comIn...  talk.politics.mideast\n",
       "4  In article <iacovou.734063606@gurney> iacovou@...  talk.politics.mideast"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    df.loc[index, 'content'] = preproces(row['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>message article stuff call give read paragraph...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>article Bonnie article even believe God case s...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>School people town know happen February school...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article get break article two make hard respon...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article even uncivilized compassion humanitari...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content                  class\n",
       "0  message article stuff call give read paragraph...  talk.politics.mideast\n",
       "1  article Bonnie article even believe God case s...  talk.politics.mideast\n",
       "2  School people town know happen February school...  talk.politics.mideast\n",
       "3  article get break article two make hard respon...  talk.politics.mideast\n",
       "4  article even uncivilized compassion humanitari...  talk.politics.mideast"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>message article stuff call give read paragraph...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>[message, article, stuff, call, give, read, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>article Bonnie article even believe God case s...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>[article, Bonnie, article, even, believe, God,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>School people town know happen February school...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>[School, people, town, know, happen, February,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article get break article two make hard respon...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>[article, get, break, article, two, make, hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article even uncivilized compassion humanitari...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>[article, even, uncivilized, compassion, human...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content                  class  \\\n",
       "0  message article stuff call give read paragraph...  talk.politics.mideast   \n",
       "1  article Bonnie article even believe God case s...  talk.politics.mideast   \n",
       "2  School people town know happen February school...  talk.politics.mideast   \n",
       "3  article get break article two make hard respon...  talk.politics.mideast   \n",
       "4  article even uncivilized compassion humanitari...  talk.politics.mideast   \n",
       "\n",
       "                                              tokens  \n",
       "0  [message, article, stuff, call, give, read, pa...  \n",
       "1  [article, Bonnie, article, even, believe, God,...  \n",
       "2  [School, people, town, know, happen, February,...  \n",
       "3  [article, get, break, article, two, make, hard...  \n",
       "4  [article, even, uncivilized, compassion, human...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new column for Word2Vec\n",
    "df['tokens'] = df['content'].apply(lambda x: x.split())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>class</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>message article stuff call give read paragraph...</td>\n",
       "      <td>6</td>\n",
       "      <td>[message, article, stuff, call, give, read, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>article Bonnie article even believe God case s...</td>\n",
       "      <td>6</td>\n",
       "      <td>[article, Bonnie, article, even, believe, God,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>School people town know happen February school...</td>\n",
       "      <td>6</td>\n",
       "      <td>[School, people, town, know, happen, February,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article get break article two make hard respon...</td>\n",
       "      <td>6</td>\n",
       "      <td>[article, get, break, article, two, make, hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article even uncivilized compassion humanitari...</td>\n",
       "      <td>6</td>\n",
       "      <td>[article, even, uncivilized, compassion, human...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  class  \\\n",
       "0  message article stuff call give read paragraph...      6   \n",
       "1  article Bonnie article even believe God case s...      6   \n",
       "2  School people town know happen February school...      6   \n",
       "3  article get break article two make hard respon...      6   \n",
       "4  article even uncivilized compassion humanitari...      6   \n",
       "\n",
       "                                              tokens  \n",
       "0  [message, article, stuff, call, give, read, pa...  \n",
       "1  [article, Bonnie, article, even, believe, God,...  \n",
       "2  [School, people, town, know, happen, February,...  \n",
       "3  [article, get, break, article, two, make, hard...  \n",
       "4  [article, even, uncivilized, compassion, human...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import label encoder \n",
    "from sklearn import preprocessing \n",
    "  \n",
    "# label_encoder object knows  \n",
    "# how to understand word labels. \n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "df['class']= label_encoder.fit_transform(df['class']) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de pesado TF y TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (805, 1672)\n",
      "Feature Names: ['ability' 'able' 'able get' ... 'young' 'zero' 'zionist']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Suponiendo que tu DataFrame se llama 'df' y la columna de texto es 'content'\n",
    "contents = df['content']\n",
    "\n",
    "# Crear un objeto TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df=0.8, ngram_range=(1, 2))\n",
    "\n",
    "# Ajustar y transformar los datos de texto a TF-IDF\n",
    "tfidf_matrix = vectorizer.fit_transform(contents)\n",
    "\n",
    "# tfidf_matrix es una matriz dispersa (sparse matrix). Si deseas convertirla a una matriz densa:\n",
    "dense_matrix = tfidf_matrix.toarray()\n",
    "\n",
    "# Opcional: Obtener los nombres de las características (palabras o términos únicos)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_matrix.shape)  # Dimensiones: (número de documentos, número de términos)\n",
    "print(\"Feature Names:\", feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Entrenar el modelo Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences=df['tokens'],  # Lista de listas de palabras\n",
    "    vector_size=200,         # Dimensión de los vectores\n",
    "    window=10,                # Contexto de palabras (tamaño de la ventana)\n",
    "    min_count=5,             # Mínimo número de ocurrencias para considerar una palabra\n",
    "    workers=8,               # Número de hilos (núcleos de CPU)\n",
    "    sg=1,                     # Skip-gram (1) o CBOW (0)\n",
    "    epochs=50        # Más épocas\n",
    ")\n",
    "\n",
    "# Guardar el modelo entrenado (opcional)\n",
    "# model.save(\"word2vec_model\")\n",
    "\n",
    "# Opcional: Cargar el modelo posteriormente\n",
    "# model = Word2Vec.load(\"word2vec_model\")\n",
    "\n",
    "# Inspeccionar las palabras más similares a un término\n",
    "# similar_words = model.wv.most_similar(\"article\", topn=5)\n",
    "# print(\"Palabras más similares a 'article':\", similar_words)\n",
    "\n",
    "# # Obtener el vector de una palabra\n",
    "# vector = model.wv['article']\n",
    "# print(\"Vector para 'article':\", vector)\n",
    "\n",
    "# Paso 1: Representar cada documento como un vector promedio usando el modelo Word2Vec\n",
    "def document_to_vector(doc, model):\n",
    "    \"\"\"\n",
    "    Convierte un documento (lista de palabras) en un vector promedio de embeddings.\n",
    "    Si el documento no tiene palabras en el vocabulario, devuelve un vector de ceros.\n",
    "    \"\"\"\n",
    "    vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "    if len(vectors) == 0:  # Si el documento no tiene palabras en el vocabulario\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# # Crear la matriz de vectores promedio para todos los documentos\n",
    "# doc_vectors = np.array([document_to_vector(doc, model) for doc in df['tokens']])\n",
    "# print(\"Shape de la matriz de vectores de documentos:\", doc_vectors.shape)\n",
    "\n",
    "# # Normalizar los vectores\n",
    "# doc_vectors = normalize(doc_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo aditivo y el modelo de la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for _, doc in df.iterrows():\n",
    "    documents.append(doc['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Obtener representaciones a nivel de documento\n",
    "additive_representations = []\n",
    "mean_representations = []\n",
    "\n",
    "# for doc in documents:\n",
    "for doc in documents:\n",
    "    # model\n",
    "    word_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "    if word_vectors:\n",
    "        # Modelo aditivo: suma de los vectores\n",
    "        additive_representation = np.sum(word_vectors, axis=0)\n",
    "        additive_representations.append(additive_representation.tolist())\n",
    "        \n",
    "        # Modelo de la media: promedio de los vectores\n",
    "        mean_representation = np.mean(word_vectors, axis=0)\n",
    "        mean_representations.append(mean_representation.tolist())\n",
    "    else:\n",
    "        # En caso de que el documento no tenga palabras en el vocabulario de Word2Vec\n",
    "        additive_representations.append([0] * model.vector_size)\n",
    "        mean_representations.append([0] * model.vector_size)\n",
    "\n",
    "# Guardar las representaciones en archivos JSON\n",
    "# additive_file = \"representaciones/additive_representation.json\"\n",
    "# mean_file = \"representaciones/mean_representation.json\"\n",
    "\n",
    "# with open(additive_file, 'w') as file:\n",
    "#     json.dump(additive_representations, file)\n",
    "\n",
    "# with open(mean_file, 'w') as file:\n",
    "#     json.dump(mean_representations, file)\n",
    "\n",
    "# print(f\"Representación aditiva guardada en {additive_file}\")\n",
    "# print(f\"Representación de la media guardada en {mean_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805\n",
      "805\n"
     ]
    }
   ],
   "source": [
    "print(len(additive_representations))\n",
    "print(len(mean_representations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def return_vectors(type_of_vector):\n",
    "    match type_of_vector:\n",
    "        case \"tfidf_matrix\":\n",
    "            # Aplicar Truncated SVD para reducir la dimensionalidad de la matriz TF-IDF\n",
    "            svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "            reduced_vectors = svd.fit_transform(dense_matrix)\n",
    "\n",
    "            normalizer = Normalizer()\n",
    "            return normalizer.fit_transform(reduced_vectors)\n",
    "            # return dense_matrix\n",
    "\n",
    "        case \"word2vec\":\n",
    "            # Crear la matriz de vectores promedio para todos los documentos\n",
    "            doc_vectors = np.array([document_to_vector(doc, model) for doc in df['tokens']])\n",
    "            print(\"Shape de la matriz de vectores de documentos:\", doc_vectors.shape)\n",
    "            # Normalizar los vectores\n",
    "            return normalize(doc_vectors)\n",
    "        \n",
    "        case \"aditive\":\n",
    "            return normalize(np.array(additive_representations))\n",
    "\n",
    "        case \"mean\":\n",
    "            return normalize(np.array(mean_representations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means\n",
    "https://www.kaggle.com/code/aybukehamideak/clustering-text-documents-using-k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels_true = df['class']\n",
    "true_k = len(np.unique(labels_true)) ## This should be 7 in this example\n",
    "print(true_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means con TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.524\n",
      "Cobertura: 0.487\n",
      "Medida-F (Bcubed): 0.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maciasalvasalva/miniconda3/envs/MachineLearning/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, adjusted_rand_score\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Ajustar KMeans con inicialización mejorada\n",
    "kmeans = KMeans(n_clusters=true_k, init='k-means++', n_init=100, max_iter=100, random_state=42)\n",
    "\n",
    "# Ajustamos el modelo a los vectores de los documentos\n",
    "labels_pred = kmeans.fit_predict(return_vectors(\"tfidf_matrix\"))\n",
    "\n",
    "# Mapear las etiquetas de los clusters a las etiquetas reales\n",
    "cluster_labels = np.zeros_like(labels_pred)\n",
    "\n",
    "for cluster in range(true_k):\n",
    "    mask = (labels_pred == cluster)\n",
    "    if np.sum(mask) == 0:\n",
    "        continue\n",
    "    cluster_labels[mask] = np.bincount(labels_true[mask]).argmax()\n",
    "\n",
    "# Calcular métricas de evaluación\n",
    "precision = precision_score(labels_true, cluster_labels, average='weighted')\n",
    "recall = recall_score(labels_true, cluster_labels, average='weighted')\n",
    "f1 = f1_score(labels_true, cluster_labels, average='weighted')\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(f\"Precisión: {precision:.3f}\")\n",
    "print(f\"Cobertura: {recall:.3f}\")\n",
    "print(f\"Medida-F (Bcubed): {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means con wprd2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de la matriz de vectores de documentos: (805, 200)\n",
      "Precisión: 0.535\n",
      "Cobertura: 0.522\n",
      "Medida-F (Bcubed): 0.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maciasalvasalva/miniconda3/envs/MachineLearning/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Ajustar KMeans con inicialización mejorada\n",
    "kmeans = KMeans(n_clusters=true_k, init='k-means++', n_init=50, max_iter=300, random_state=42)\n",
    "\n",
    "# Ajustamos el modelo a los vectores de los documentos\n",
    "labels_pred = kmeans.fit_predict(return_vectors(\"word2vec\"))\n",
    "\n",
    "# Mapear las etiquetas de los clusters a las etiquetas reales\n",
    "cluster_labels = np.zeros_like(labels_pred)\n",
    "\n",
    "for cluster in range(true_k):\n",
    "    mask = (labels_pred == cluster)\n",
    "    if np.sum(mask) == 0:\n",
    "        continue\n",
    "    cluster_labels[mask] = np.bincount(labels_true[mask]).argmax()\n",
    "\n",
    "# Calcular métricas de evaluación\n",
    "precision = precision_score(labels_true, cluster_labels, average='weighted')\n",
    "recall = recall_score(labels_true, cluster_labels, average='weighted')\n",
    "f1 = f1_score(labels_true, cluster_labels, average='weighted')\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(f\"Precisión: {precision:.3f}\")\n",
    "print(f\"Cobertura: {recall:.3f}\")\n",
    "print(f\"Medida-F (Bcubed): {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means con aditivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.512\n",
      "Cobertura: 0.542\n",
      "Medida-F (Bcubed): 0.499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maciasalvasalva/miniconda3/envs/MachineLearning/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Ajustar KMeans con inicialización mejorada\n",
    "kmeans = KMeans(n_clusters=true_k, init='k-means++', n_init=200, max_iter=500, random_state=42, algorithm='elkan')\n",
    "\n",
    "# Ajustamos el modelo a los vectores de los documentos\n",
    "labels_pred = kmeans.fit_predict(return_vectors(\"aditive\"))\n",
    "\n",
    "# Mapear las etiquetas de los clusters a las etiquetas reales\n",
    "cluster_labels = np.zeros_like(labels_pred)\n",
    "\n",
    "for cluster in range(true_k):\n",
    "    mask = (labels_pred == cluster)\n",
    "    if np.sum(mask) == 0:\n",
    "        continue\n",
    "    cluster_labels[mask] = np.bincount(labels_true[mask]).argmax()\n",
    "\n",
    "# Calcular métricas de evaluación\n",
    "precision = precision_score(labels_true, cluster_labels, average='weighted')\n",
    "recall = recall_score(labels_true, cluster_labels, average='weighted')\n",
    "f1 = f1_score(labels_true, cluster_labels, average='weighted')\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(f\"Precisión: {precision:.3f}\")\n",
    "print(f\"Cobertura: {recall:.3f}\")\n",
    "print(f\"Medida-F (Bcubed): {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means con media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.417\n",
      "Cobertura: 0.512\n",
      "Medida-F (Bcubed): 0.418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maciasalvasalva/miniconda3/envs/MachineLearning/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Ajustar KMeans con inicialización mejorada\n",
    "kmeans = KMeans(n_clusters=true_k, init='k-means++', n_init=50, max_iter=300, random_state=42)\n",
    "\n",
    "# Ajustamos el modelo a los vectores de los documentos\n",
    "labels_pred = kmeans.fit_predict(return_vectors(\"mean\"))\n",
    "\n",
    "# Mapear las etiquetas de los clusters a las etiquetas reales\n",
    "cluster_labels = np.zeros_like(labels_pred)\n",
    "\n",
    "for cluster in range(true_k):\n",
    "    mask = (labels_pred == cluster)\n",
    "    if np.sum(mask) == 0:\n",
    "        continue\n",
    "    cluster_labels[mask] = np.bincount(labels_true[mask]).argmax()\n",
    "\n",
    "# Calcular métricas de evaluación\n",
    "precision = precision_score(labels_true, cluster_labels, average='weighted')\n",
    "recall = recall_score(labels_true, cluster_labels, average='weighted')\n",
    "f1 = f1_score(labels_true, cluster_labels, average='weighted')\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(f\"Precisión: {precision:.3f}\")\n",
    "print(f\"Cobertura: {recall:.3f}\")\n",
    "print(f\"Medida-F (Bcubed): {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model (GMM)\n",
    "## GMM tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.328\n",
      "Cobertura: 0.401\n",
      "Medida-F (Bcubed): 0.334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maciasalvasalva/miniconda3/envs/MachineLearning/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Paso 1: Ajustar Gaussian Mixture Model\n",
    "# 'true_k' es el número de clusters reales, 'doc_vectors' es la matriz de vectores de documentos\n",
    "gmm = GaussianMixture(n_components=true_k, covariance_type='full', random_state=42)\n",
    "\n",
    "# Ajustamos el modelo a los vectores de los documentos\n",
    "labels_pred = gmm.fit_predict(return_vectors(\"tfidf_matrix\"))\n",
    "\n",
    "# Paso 2: Mapear las etiquetas de los clusters a las etiquetas reales\n",
    "cluster_labels = np.zeros_like(labels_pred)\n",
    "\n",
    "# Aquí mapeamos cada cluster a la etiqueta más frecuente dentro de ese cluster\n",
    "for cluster in range(true_k):\n",
    "    mask = (labels_pred == cluster)\n",
    "    if np.sum(mask) == 0:  # Si el cluster está vacío, saltamos\n",
    "        continue\n",
    "    cluster_labels[mask] = np.bincount(labels_true[mask]).argmax()\n",
    "\n",
    "# Paso 3: Calcular las métricas de evaluación\n",
    "# Calculamos precisión, recall y f1-score usando 'cluster_labels' (etiquetas asignadas por el clustering)\n",
    "precision = precision_score(labels_true, cluster_labels, average='weighted')\n",
    "recall = recall_score(labels_true, cluster_labels, average='weighted')\n",
    "f1 = f1_score(labels_true, cluster_labels, average='weighted')\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(f\"Precisión: {precision:.3f}\")\n",
    "print(f\"Cobertura: {recall:.3f}\")\n",
    "print(f\"Medida-F (Bcubed): {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de la matriz de vectores de documentos: (805, 200)\n",
      "Precisión: 0.535\n",
      "Cobertura: 0.522\n",
      "Medida-F (Bcubed): 0.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maciasalvasalva/miniconda3/envs/MachineLearning/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Ajustar Gaussian Mixture Model\n",
    "# 'true_k' es el número de clusters reales, 'doc_vectors' es la matriz de vectores de documentos\n",
    "gmm = GaussianMixture(n_components=true_k, covariance_type='full', random_state=42)\n",
    "\n",
    "# Ajustamos el modelo a los vectores de los documentos\n",
    "labels_pred = gmm.fit_predict(return_vectors(\"word2vec\"))\n",
    "\n",
    "# Paso 2: Mapear las etiquetas de los clusters a las etiquetas reales\n",
    "cluster_labels = np.zeros_like(labels_pred)\n",
    "\n",
    "# Aquí mapeamos cada cluster a la etiqueta más frecuente dentro de ese cluster\n",
    "for cluster in range(true_k):\n",
    "    mask = (labels_pred == cluster)\n",
    "    if np.sum(mask) == 0:  # Si el cluster está vacío, saltamos\n",
    "        continue\n",
    "    cluster_labels[mask] = np.bincount(labels_true[mask]).argmax()\n",
    "\n",
    "# Paso 3: Calcular las métricas de evaluación\n",
    "# Calculamos precisión, recall y f1-score usando 'cluster_labels' (etiquetas asignadas por el clustering)\n",
    "precision = precision_score(labels_true, cluster_labels, average='weighted')\n",
    "recall = recall_score(labels_true, cluster_labels, average='weighted')\n",
    "f1 = f1_score(labels_true, cluster_labels, average='weighted')\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(f\"Precisión: {precision:.3f}\")\n",
    "print(f\"Cobertura: {recall:.3f}\")\n",
    "print(f\"Medida-F (Bcubed): {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM aditive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.442\n",
      "Cobertura: 0.542\n",
      "Medida-F (Bcubed): 0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maciasalvasalva/miniconda3/envs/MachineLearning/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Ajustar Gaussian Mixture Model\n",
    "# 'true_k' es el número de clusters reales, 'doc_vectors' es la matriz de vectores de documentos\n",
    "gmm = GaussianMixture(n_components=true_k, covariance_type='full', random_state=42)\n",
    "\n",
    "# Ajustamos el modelo a los vectores de los documentos\n",
    "labels_pred = gmm.fit_predict(return_vectors(\"aditive\"))\n",
    "\n",
    "# Paso 2: Mapear las etiquetas de los clusters a las etiquetas reales\n",
    "cluster_labels = np.zeros_like(labels_pred)\n",
    "\n",
    "# Aquí mapeamos cada cluster a la etiqueta más frecuente dentro de ese cluster\n",
    "for cluster in range(true_k):\n",
    "    mask = (labels_pred == cluster)\n",
    "    if np.sum(mask) == 0:  # Si el cluster está vacío, saltamos\n",
    "        continue\n",
    "    cluster_labels[mask] = np.bincount(labels_true[mask]).argmax()\n",
    "\n",
    "# Paso 3: Calcular las métricas de evaluación\n",
    "# Calculamos precisión, recall y f1-score usando 'cluster_labels' (etiquetas asignadas por el clustering)\n",
    "precision = precision_score(labels_true, cluster_labels, average='weighted')\n",
    "recall = recall_score(labels_true, cluster_labels, average='weighted')\n",
    "f1 = f1_score(labels_true, cluster_labels, average='weighted')\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(f\"Precisión: {precision:.3f}\")\n",
    "print(f\"Cobertura: {recall:.3f}\")\n",
    "print(f\"Medida-F (Bcubed): {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.532\n",
      "Cobertura: 0.511\n",
      "Medida-F (Bcubed): 0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maciasalvasalva/miniconda3/envs/MachineLearning/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Ajustar Gaussian Mixture Model\n",
    "# 'true_k' es el número de clusters reales, 'doc_vectors' es la matriz de vectores de documentos\n",
    "gmm = GaussianMixture(n_components=true_k, covariance_type='full', random_state=42)\n",
    "\n",
    "# Ajustamos el modelo a los vectores de los documentos\n",
    "labels_pred = gmm.fit_predict(return_vectors(\"mean\"))\n",
    "\n",
    "# Paso 2: Mapear las etiquetas de los clusters a las etiquetas reales\n",
    "cluster_labels = np.zeros_like(labels_pred)\n",
    "\n",
    "# Aquí mapeamos cada cluster a la etiqueta más frecuente dentro de ese cluster\n",
    "for cluster in range(true_k):\n",
    "    mask = (labels_pred == cluster)\n",
    "    if np.sum(mask) == 0:  # Si el cluster está vacío, saltamos\n",
    "        continue\n",
    "    cluster_labels[mask] = np.bincount(labels_true[mask]).argmax()\n",
    "\n",
    "# Paso 3: Calcular las métricas de evaluación\n",
    "# Calculamos precisión, recall y f1-score usando 'cluster_labels' (etiquetas asignadas por el clustering)\n",
    "precision = precision_score(labels_true, cluster_labels, average='weighted')\n",
    "recall = recall_score(labels_true, cluster_labels, average='weighted')\n",
    "f1 = f1_score(labels_true, cluster_labels, average='weighted')\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(f\"Precisión: {precision:.3f}\")\n",
    "print(f\"Cobertura: {recall:.3f}\")\n",
    "print(f\"Medida-F (Bcubed): {f1:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
