{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen Teorico del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McNemar’s Test for Correlated Proportions (Q. McNemar, 1947)\n",
    "- Contexto y Propósito: La prueba de McNemar fue introducida para comparar dos proporciones o porcentajes en situaciones donde las observaciones están correlacionadas, es decir, provienen de muestras relacionadas o dependientes. Este escenario es típico cuando se desea evaluar cambios en respuestas de un mismo grupo antes y después de una intervención, o al comparar dos clasificaciones en el mismo conjunto de sujetos.\n",
    "\n",
    "- Método: La prueba de McNemar se basa en analizar los pares discordantes entre dos condiciones (A y B), en los que un sujeto tiene respuestas diferentes entre ambas condiciones (por ejemplo, clasificado positivo en A y negativo en B, o viceversa). Solo se consideran estos casos discordantes, lo que permite calcular si la diferencia observada es significativa sin asumir independencia entre las muestras.\n",
    "\n",
    "- Aplicación en Aprendizaje Automático: Este método es útil en la evaluación de clasificadores de aprendizaje automático, permitiendo comparar el desempeño de dos modelos sobre el mismo conjunto de datos para determinar si las diferencias en las tasas de clasificación son estadísticamente significativas.\n",
    "\n",
    "### Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms (T. G. Dietterich, 1998)\n",
    "- Problema de Comparación de Algoritmos: En su trabajo, Dietterich analiza métodos estadísticos para evaluar diferencias en el rendimiento de algoritmos de clasificación. Dado que los resultados en aprendizaje automático pueden variar debido a la aleatoriedad del muestreo y de los modelos, es crucial contar con pruebas estadísticas adecuadas para evitar conclusiones engañosas.\n",
    "\n",
    "- Pruebas Analizadas:\n",
    "    - Prueba t de Diferencia Pareada en k-fold cross-validation, un método común pero que puede producir falsos positivos debido a las dependencias entre pliegues en el conjunto de datos.\n",
    "\n",
    "    - Prueba t con 5x2 cross-validation: En respuesta a las limitaciones de k-fold, Dietterich propone esta alternativa, que consiste en realizar cinco repeticiones de un proceso de 2-fold cross-validation. Este método reduce la varianza y el sesgo al manejar mejor las dependencias en los datos, resultando en una evaluación estadística más robusta.\n",
    "\n",
    "    - Prueba de McNemar: Dietterich también revisa la prueba de McNemar como una opción válida en casos donde se comparan directamente las clasificaciones correctas e incorrectas de los modelos sobre el mismo conjunto de datos.\n",
    "\n",
    "- Recomendaciones: Dietterich concluye que la prueba 5x2 cross-validation t-test es una de las más confiables para comparar modelos de clasificación, sugiriendo su uso en lugar de pruebas tradicionales de k-fold para minimizar errores tipo I (falsos positivos).\n",
    "\n",
    "\n",
    "### Q. McNemar, “Note on the sampling error of the difference between corre- lated proportions or percentages,” Psychometrika, vol. 12, pp. 153–157, Jun 1947.\n",
    "\n",
    "El test de McNemar nos permite identificar cambios en variables categóricas a lo largo del tiempo. Para detectar estos cambios, necesitamos dos mediciones: una antes de la intervención y otra después. Un ejemplo teórico es:\n",
    "\n",
    "|                         | Medicamento B postivo | Medicamento B negativo |\n",
    "|-------------------------|-----------------------|------------------------|\n",
    "| Medicamento A Positivo  | a                     | b                      |\n",
    "| Medicamento A Negativo  | c                     | d                      |\n",
    "\n",
    "- a: Número de casos positivos en ambos tratamientos, en A y B.\n",
    "- b: Número de casos positivos en A y negativos en B.\n",
    "- c: Número de casos negativos en A y positivos en B.\n",
    "- d: Número de casos negativos en ambos tratamientos.\n",
    "\n",
    "A continuación, definiremos dos hipótesis para nuestro estudio:\n",
    "- Hipótesis Nula (H₀). No hay diferencia entre el Medicamento A y el Medicamento B.\n",
    "- Hipótesis Alternativa (H₁): Existe una diferencia entre la efectividad de los medicamentos. \n",
    "\n",
    "Continuemos con el cálculo del estadístico de prueba utilizando el test de McNemar para nuestro ejemplo: $ x^{2} = \\frac{(b-c)^{2}}{b+c} $\n",
    "\n",
    "Finalmente, sabemos que nuestro test de McNemar se basa en una distribución chi-cuadrado χ². A partir de esta distribución, determinaremos cuál de los tratamientos es más efectivo.\n",
    "\n",
    "En nuestro enfoque en inteligencia artificial, el test de McNemar puede ser especialmente interesante al aplicarlo a diferentes modelos de aprendizaje automático. Esto nos permite evaluar si hay diferencias significativas en el rendimiento de los modelos al clasificar los mismos conjuntos de datos.\n",
    "|                       | Algoritmo B postivo   | Algoritmo B negativo |\n",
    "|-----------------------|-----------------------|----------------------|\n",
    "| Algoritmo A Positivo  | a                     | b                    |\n",
    "| Algoritmo A Negativo  | c                     | d                    |\n",
    "\n",
    "\n",
    "El test de McNemar es ampliamente aplicado en estudios en psicología, medicina y ciencias sociales, donde se analiza el cambio en la respuesta de los mismos sujetos, como estudios pre y post intervención. Por ejemplo, se usa para evaluar la efectividad de tratamientos médicos, cambios en comportamientos, o la modificación de actitudes tras un programa educativo. Al enfocarse en los cambios entre las dos mediciones, el test de McNemar reduce el riesgo de error al no suponer que las observaciones son independientes, lo cual mejora la precisión en estudios longitudinales o de diseño experimental.\n",
    "\n",
    "\n",
    "### Conclusiones Combinadas\n",
    "Ambos trabajos proponen métodos específicos para enfrentar la comparación de algoritmos en condiciones de dependencia entre muestras. La prueba de McNemar se centra en comparar proporciones correlacionadas y es particularmente útil en contextos donde las clasificaciones de los modelos pueden analizarse como pares. Por otro lado, Dietterich expande el análisis al proponer técnicas de validación cruzada que ofrecen evaluaciones más precisas y recomendables para estudios en aprendizaje automático, donde el uso de métodos inadecuados podría llevar a conclusiones erróneas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tecnicas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create a DataFrame from the dataset for better readability\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7 \n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(f\"Accuracy: {results.mean()*100.0:,.2F}% ({results.std()*100.0:,.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripcion de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
