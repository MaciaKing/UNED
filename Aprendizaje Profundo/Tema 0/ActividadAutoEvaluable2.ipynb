{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 6\n",
    "6.Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img width=\"50%\" src=\"images/Red1.png\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the shape of the input matrix X?\n",
    "\n",
    "mx10. Where m is single data sample or instance\n",
    "<div style=\"text-align: center;\">\n",
    "    X∈R^(m×10)\n",
    "</div>\n",
    "\n",
    "- What are the shapes of the hidden layer’s weight matrix Wh and bias vector bh?\n",
    "10 de la input layer y 50 de la hidden layer. Y 50bh. \n",
    "<div style=\"text-align: center;\">\n",
    "    10x50\n",
    "</div>\n",
    "\n",
    "- What are the shapes of the output layer’s weight matrix Wo and bias vector bo?\n",
    "50 de la hidden layer y 3 de la output layer. Bo hay 3.\n",
    "<div style=\"text-align: center;\">\n",
    "    50x10\n",
    "</div>\n",
    "\n",
    "- What is the shape of the network’s output matrix Y?\n",
    "<div style=\"text-align: center;\">\n",
    "    mx3\n",
    "</div>\n",
    "\n",
    "- Write the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo, and bo.\n",
    "<div style=\"text-align: center;\">\n",
    "    Y = Relu(Relu(X * Wh + bh) W0 + b0)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 7\n",
    "How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, and which activation function should you use? What about for getting your network to predict housing prices, as in Chapter 2?\n",
    "\n",
    "- Para la clasificacion de spam o no spam necesitare 1 neurona. La funcion de activavion que utilizaria seria una RELU ya que tratamos de decir si es una classe SPAM o no SPAM. Ya que solo queremos decir si es una classe u otra necesitaremos una probabilidad que nos diga si es una u otra, por esto utilizaremos la funcion Sigmoid.\n",
    "\n",
    "- Para el dataset de MNIST tenemos 10 clases a predecir, del 0 al 9. Entonces como salida tendre 10 neuronas que seran las posibles classes con cada una una probabilidad, para esto utilizare la funcion de activacion SoftMax.\n",
    "\n",
    "- Para predecir el precio de las casas sera un numero, el precio. Para esto tendre una neurona como salida y necesitare una ecuacion lineal (SIN FUNCION DE ACTIVACION).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 8\n",
    "What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "\n",
    "- El backpropagation es un método de optimización que ajusta los pesos de una red neuronal basada en el error de las predicciones realizadas. Este proceso permite que la red aprenda de sus errores y mejore su desempeño a lo largo del tiempo.\n",
    "\n",
    "- El backpropagation es una aplicación específica de la autodiferenciación en modo reverso (reverse-mode autodiff) diseñada para entrenar redes neuronales, calculando gradientes de manera eficiente al aprovechar la estructura de capas de estas redes. Por otro lado, la autodiferenciación en modo reverso es una técnica más general que se puede aplicar a cualquier función matemática para calcular derivadas, siendo especialmente útil cuando hay muchas variables de entrada y una sola salida, aunque no está optimizada específicamente para la estructura de redes neuronales como el backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 9\n",
    "Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "\n",
    "- Here is a list of all the hyperparameters you can tweak in a basic MLP: the number of hidden layers, the number of neurons in each hidden layer, and the activation function used in each hidden layer and in the output layer. In general, the ReLU activation function (or one of its variants; see Chapter 11) is a good default for the hidden layers. For the output layer, in general you will want the sigmoid activation function for binary classification, the softmax activation function for multiclass classification, or no activation function for regression. If the MLP overfits the training data, you can try reducing the number of hidden layers and reducing the number of neurons per hidden layer."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
